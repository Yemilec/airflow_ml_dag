# Machine Learning Pipeline exercice - Spin by Oxxo

This test consisted of translating a kernel source code to an Airflow pipeline (DAG).
The kernel on which this exercise was based can be found at the following [link](https://www.kaggle.com/code/kabure/predicting-credit-risk-model-pipeline/notebook)

The DAG consists of reading a dataset, generating and transforming variables necessary for the analysis, applying various machine learning models and writing the respective predictions in parquet format.

## Test

The repository consists of the 'ml_pipeline.py' file into the dags folder and the data folder which stores the necessary input and output generated by the DAG. 
Both folders (*dags* and *data*) must be into the airflow home (i.e. *~/airflow/*).

Use the command line to do metadata validation.
In the terminal run 
```
airflow tasks list ml_pipeline --tree
```
to print the hierarchy of tasks in the DAG.

To test a simple task:
```
airflow tasks test ml_pipeline task date
```
where task must be one of the next:
    loading_data, preprocess_data, split_data, prepare_model, first_model, second_model, pipeline_model or last_model
And date the execution date (i.e. 2023-01-27)

To validate that the outputs files were generated correctly
```
ls ~/airflow/data/output/german-credit-data-with-risk/parquets/
```

## To Do
Refactor methods with design patterns.

Unit test.

Assuming that this data frame was to be used later for data analysis or as a data source for some other process, we should consider moving the data to a database where it might be convenient to consider some logical or physical partitioning of this, depending on the use and frequency with which it is consulted.
